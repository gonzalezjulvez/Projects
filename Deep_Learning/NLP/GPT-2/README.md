# Generación del lenguaje con GPT2

GPT-2 se entrenó simplemente para predecir la siguiente palabra en 40 GB de texto de Internet.GPT-2 es un gran modelo de lenguaje basado en transformadores con 1.500 millones de parámetros, entrenado en un conjunto de datos de 8 millones de páginas web. GPT-2 se entrena con un objetivo simple: predecir la siguiente palabra, dadas todas las palabras anteriores dentro de un texto. La diversidad del conjunto de datos hace que este simple objetivo contenga demostraciones naturales de muchas tareas en diversos dominios. GPT-2 es un escalado directo de GPT, con más de 10 veces los parámetros y entrenado en más de 10 veces la cantidad de datos. En tareas de lenguaje como respuesta a preguntas, comprensión de lectura, resumen y traducción, GPT-2 comienza a aprender estas tareas a partir del texto sin procesar, sin utilizar datos de entrenamiento específicos de la tarea. Si bien los puntajes en estas tareas posteriores están lejos de ser el estado de la técnica, sugieren que las tareas pueden beneficiarse de técnicas no supervisadas, con suficientes datos y computación (sin etiquetar).