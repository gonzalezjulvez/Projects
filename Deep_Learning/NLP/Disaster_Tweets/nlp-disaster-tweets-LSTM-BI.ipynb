{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torchtext\nfrom pathlib import Path\nimport matplotlib.pyplot as plt \nimport sklearn.metrics as m\nfrom transformers import BertTokenizer \nimport warnings  \nwarnings.filterwarnings('ignore')\ntorch.__version__ , torchtext.__version__","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"PATH = Path(\"/kaggle/input/nlp-disaster-tweets-eda\")\nos.listdir(PATH)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#importamos los datos de entrenamientos\n\ntrain = pd.read_csv(PATH/\"train.csv\")\ntrain.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Los transformers estan limitados en cuanto a la longitud de palabras a leer y es por ello que deberemos tenerlo en cuenta \n\nmax_input_length = tokenizer.max_model_input_sizes[\"bert-base-uncased\"] # Bert trabaja con 512.\n\ndef tokenize_and_cut(sentence):\n    tokens = tokenizer.tokenize(sentence)\n    tokens = tokens[:max_input_length-2] #Deberemos restarle dos por los tokens unk y pad\n    return tokens","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ID = torchtext.data.RawField()\nKEYWORD = torchtext.data.RawField()\nLOCATION = torchtext.data.RawField()\nTEXT = torchtext.data.Field(batch_first=True,\n                            use_vocab=False,\n                            tokenize= tokenize_and_cut,\n                            preprocessing = tokenizer.convert_tokens_to_ids,\n                            init_token = tokenizer.cls_token_id,\n                            eos_token = tokenizer.sep_token_id,\n                            pad_token = tokenizer.pad_token_id,\n                            unk_token = tokenizer.unk_token_id)\n \nLABEL = torchtext.data.LabelField(dtype=torch.long)\n\ndataset = torchtext.data.TabularDataset(\n    path=PATH / 'train.csv',\n    format = \"CSV\",\n    fields = [(\"id\",ID),(\"keyword\",KEYWORD),(\"location\",LOCATION),(\"text\",TEXT),(\"target\",LABEL)],\n    skip_header=True\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(dataset)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ix=0\nprint(vars(dataset.examples[ix]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset, valid_dataset = dataset.split(\n    split_ratio = 0.6,\n    stratified =True,\n    strata_field=\"target\"\n)\nlen(train_dataset), len(valid_dataset)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Proceso de tokenizacion\n#MAX_VOCAB_SIZE = 10000\n#TEXT.build_vocab(train_dataset, # construiremos nuestro vocabulario del tokenizador\n#                 max_size = MAX_VOCAB_SIZE,\n#                 vectors = \"glove.6B.100d\", # el parametro vectors nos permite obtener embedding ya entrenados\n#                 unk_init = torch.Tensor.normal_) \nLABEL.build_vocab(train_dataset)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(LABEL.vocab)\n# Esta limitado a 10000 pero ha creado dos tokens extra, uno es el unk que se lo pondrá a las palabras que no han entrado en nuestro diccionario por ser poco frecuentes, el segundo es el PAD que se trata de un token vacio que permite igualar las dimensiones de las frases","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# freqs nos devolvera los tokens con las veces que aparecen en nuestro datasets\nTEXT.vocab.freqs.most_common(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BATCH_SIZE = 64\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndataloader = {\n    \"train\": torchtext.data.BucketIterator(train_dataset, batch_size=BATCH_SIZE, shuffle=True, device=DEVICE),\n    \"val\":  torchtext.data.BucketIterator(valid_dataset, batch_size=200, device=DEVICE) # Ponemos un mayor batch_size porque no tenemos que calcular gradientes y podremos ir mas rapido\n    \n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Modelo de LSTM cargando embeddings","metadata":{}},{"cell_type":"code","source":"# CONSTRUIMOS RED NEURONAL\nclass LSTM(torch.nn.Module):\n    def __init__(self, input_dim, embedding_dim=100, hidden_size=128, num_layers=2, n_outputs=2, bidirectional=False, dropout=0):\n        super().__init__()\n        self.embedding = torch.nn.Embedding(input_dim, embedding_dim) # Cogera una palabra y creará un vector que la represente\n        self.rnn = torch.nn.LSTM( # Cargamos una red LSTM\n            input_size= embedding_dim, \n            hidden_size = hidden_size,\n            num_layers=num_layers,\n            bidirectional=bidirectional,\n            dropout= dropout)\n        self.fc = torch.nn.Linear(2*hidden_size if bidirectional else hidden_size, n_outputs)\n    \n    def forward(self, text):\n        embedded = self.embedding(text)#Le llega una frase un nos devolverá un vector de embedding_dim(128) dimensiones\n        output, _ = self.rnn(embedded) # La capa de RNN nos devolvera los outputs y el valor del ultimo hidden state que no lo queremos para nada\n        # Dimensiones en RNN y torchtext son [\"longitud del texto\",\"batch_size\",\"hidden_size\"]\n        return self.fc(output[-1,:,:].squeeze(0)) # output[-1,:,:] Nos quedaremos con la ultima palabra, todo el batch_size y el hidden_size osea tenemos [1,64,128] dimensiones luego se lo pasaremos a la capa lineal quitandole la ultima palabra con la funcion squeeze(0)       ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# PROBAMOS NUESTRO MODELO\nmodel = LSTM(input_dim=len(TEXT.vocab), bidirectional = False) # las dimensiones seran los 10002 palabras que tenemos en nuestro vocab\noutput = model ( torch.randint(0, len(TEXT.vocab), (100,64)))\noutput.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# CREAMOS LA FUNCION PARA ENTRENAR NUESTRO MODELO\n\ndef fit(model, dataloader, epochs = 10, lr=1e-3):\n    \n    model.to(DEVICE) #Mandamos nuestro modelo a la gpu\n    criterion = torch.nn.CrossEntropyLoss() # Funcion de perdida CrossEntropyLoss por que queremos mas de una salida sino seria BCE\n    optimizer = torch.optim.Adam(model.parameters(), lr) # Optimizador con los parametros del modelo y añadimos nuestra lr\n    \n    hist = {\"loss\" : [], \"f1\": [], \"val_loss\":[],\"val_f1\":[]} # Guardaremos las metricas\n    best_f1 = 0.\n    for e in range(1, epochs+1):\n        \n        # ENTRENAMOS\n        model.train() # Ponemos el model en modo entrenamiento\n        l, f1s = [], []\n        for batch in dataloader[\"train\"]:\n            optimizer.zero_grad() # Ponemos a cero los gradientes\n            y_pred = model(batch.text) # Calculamos salida pasando al modelo el texto\n            loss = criterion(y_pred, batch.target) # calulo loss functions\n            l.append(loss.item()) # guardamos loss function\n            loss.backward()#calculo gradiente\n            optimizer.step()# actualizo los pesos\n            y_pred= torch.argmax(y_pred, axis=1)# cogera el indice del valor mas grande, es decir dará 0 o 1\n            f1s.append(m.f1_score(batch.target.cpu(), y_pred.cpu()))\n        hist[\"loss\"].append(np.mean(l))\n        hist[\"f1\"].append(np.mean(f1s))\n        \n        #EVALUAMOS\n        model.eval()\n        l,acc,f1s = [],[],[]\n        with torch.no_grad():\n            for batch in dataloader[\"val\"]:\n                y_pred = model(batch.text)\n                loss = criterion (y_pred, batch.target)\n                l.append(loss.item())\n                y_pred = torch.argmax(y_pred, axis=1)\n                f1s.append(m.f1_score(batch.target.cpu(), y_pred.cpu())) # Si trabajas con el paquete sklearn los datos deben estar en la cpu y en formato numpy\n        hist[\"val_loss\"].append(np.mean(l))\n        hist[\"val_f1\"].append(np.mean(f1s))\n        # CALLBACKS SAVE BEST MODEL\n        if hist[\"val_f1\"][-1] > best_f1:\n            best_f1 = hist[\"val_f1\"][-1]\n            torch.save(model.state_dict(),\"ckpt.pt\")\n        print(f'Epoch {e}/{epochs} loss:{hist[\"loss\"][-1]:.5f} f1:{hist[\"f1\"][-1]:.5f} val_loss:{hist[\"val_loss\"][-1]:.5f} val_f1:{hist[\"val_f1\"][-1]:.5f}')\n    model.load_state_dict(torch.load(\"ckpt.pt\"))\n    return hist","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# INSTANCIAMOS LA RNN Y ENTRENAMOS\nmodel = LSTM(input_dim=len(TEXT.vocab))\n\npretrained_embeddings = TEXT.vocab.vectors\nmodel.embedding.weight.data.copy_(pretrained_embeddings)# Copiamos los embedding descargados y los metemos en nuestro modelo\nmodel.embedding.weight.data[TEXT.vocab.stoi[TEXT.unk_token]]=torch.zeros(100)\nmodel.embedding.weight.data[TEXT.vocab.stoi[TEXT.pad_token]]=torch.zeros(100)\n\n\nhist = fit(model, dataloader)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot(hist): # Funcion para graficar nuestras metricas \n    fig = plt.figure(dpi = 200, figsize= (10,3))\n    ax = plt.subplot(121)\n    hist = pd.DataFrame(hist)\n    hist[[\"loss\",\"val_loss\"]].plot(ax=ax, grid=True)\n    ax = plt.subplot(122)\n    hist[[\"f1\", \"val_f1\"]].plot(ax=ax, grid=True)\n    plt.show()\nplot(hist)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Estos malos resultados es porque las RNN sencillas no funcionan bien con longuitudes grandes. Van bien con secuancias de texto de 10-20 pero en nuestro caso podemos tener unas secuencias mayor a 100, entonces estas redes tan sencillas fallan mucho. Deberemos buscar mejores modelos como LSTM, bidireccionales o transformers","metadata":{}},{"cell_type":"markdown","source":"## TEST","metadata":{}},{"cell_type":"code","source":"# Creamos nuestros dataset de test\ntest_dataset = torchtext.data.TabularDataset(\n    path=PATH/'test.csv',\n    format = \"CSV\",\n    fields = [(\"id\",ID),(\"keyword\",KEYWORD),(\"location\",LOCATION),(\"text\",TEXT)],\n    skip_header=True\n)\nlen(test_dataset)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Comprobamos si es testo correcto\nix=3258\nprint(vars(test_dataset.examples[ix]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dataloader = torchtext.data.BucketIterator(test_dataset, batch_size=BATCH_SIZE, shuffle=False, device=DEVICE)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict():\n    model.eval()\n    preds = torch.tensor([]).to(DEVICE)\n    with torch.no_grad():\n        for batch in test_dataloader:\n            y_pred=model(batch.text)\n            y_pred = torch.argmax(y_pred, axis=1)\n            preds = torch.cat([preds, y_pred])\n    return preds","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = predict()\npreds","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = pd.read_csv(\"/kaggle/input/nlp-getting-started/sample_submission.csv\")\nsubmission.target = preds.cpu().long()# Me traigo mis pres a la cpu \nsubmission.to_csv(\"submission.csv\", index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}