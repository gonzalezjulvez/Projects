{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torchtext\nfrom pathlib import Path\nimport matplotlib.pyplot as plt \nimport sklearn.metrics as m\nfrom transformers import BertTokenizer, BertModel \nimport warnings\nfrom tqdm import tqdm\nwarnings.filterwarnings('ignore')\ntorch.__version__ , torchtext.__version__","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":32,"outputs":[{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"('1.7.0', '0.8.0a0+cd6902d')"},"metadata":{}}]},{"cell_type":"code","source":"PATH = Path(\"/kaggle/input/nlp-disaster-tweets-eda\")\nos.listdir(PATH)","metadata":{"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"['__results__.html',\n 'train_clean.csv',\n 'test_clean.csv',\n '__notebook__.ipynb',\n '__results___files',\n '__output__.json',\n 'train.csv',\n 'test.csv',\n 'custom.css']"},"metadata":{}}]},{"cell_type":"code","source":"#importamos los datos de entrenamientos\n\ntrain = pd.read_csv(PATH/\"train.csv\")\ntrain.info()","metadata":{"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 7613 entries, 0 to 7612\nData columns (total 5 columns):\n #   Column    Non-Null Count  Dtype \n---  ------    --------------  ----- \n 0   id        7613 non-null   int64 \n 1   keyword   7552 non-null   object\n 2   location  5080 non-null   object\n 3   text      7613 non-null   object\n 4   target    7613 non-null   int64 \ndtypes: int64(2), object(3)\nmemory usage: 297.5+ KB\n","output_type":"stream"}]},{"cell_type":"code","source":"# Importamos el transformers de Bert\ntokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")","metadata":{"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"12a88d83002d46879263afe6d932743c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"10fe9385802f4f56bb3e3c2e046bc3ab"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"347b386ac6674ba28398ef5fbd925e79"}},"metadata":{}}]},{"cell_type":"code","source":"# Los transformers estan limitados en cuanto a la longitud de palabras a leer y es por ello que deberemos tenerlo en cuenta \n\nmax_input_length = tokenizer.max_model_input_sizes[\"bert-base-uncased\"] # Bert trabaja con 512.\n\ndef tokenize_and_cut(sentence):\n    tokens = tokenizer.tokenize(sentence)\n    tokens = tokens[:max_input_length-2] #Deberemos restarle dos por los tokens unk y pad\n    return tokens","metadata":{"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"ID = torchtext.data.RawField()\nKEYWORD = torchtext.data.RawField()\nLOCATION = torchtext.data.RawField()\nTEXT = torchtext.data.Field(batch_first=True,# Dimension del batch en la primera dimension\n                            use_vocab=False,\n                            tokenize= tokenize_and_cut,\n                            preprocessing = tokenizer.convert_tokens_to_ids,\n                            init_token = tokenizer.cls_token_id,\n                            eos_token = tokenizer.sep_token_id,\n                            pad_token = tokenizer.pad_token_id,\n                            unk_token = tokenizer.unk_token_id)\n \nLABEL = torchtext.data.LabelField(dtype=torch.long)\n\ndataset = torchtext.data.TabularDataset(\n    path=PATH / 'train.csv',\n    format = \"CSV\",\n    fields = [(\"id\",ID),(\"keyword\",KEYWORD),(\"location\",LOCATION),(\"text\",TEXT),(\"target\",LABEL)],\n    skip_header=True\n)","metadata":{"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"len(dataset)","metadata":{"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"7613"},"metadata":{}}]},{"cell_type":"code","source":"ix=0\nprint(vars(dataset.examples[ix]))","metadata":{"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"{'id': '1', 'keyword': '', 'location': '', 'text': [2256, 15616, 2024, 1996, 3114, 1997, 2023, 1001, 8372, 2089, 16455, 9641, 2149, 2035], 'target': '1'}\n","output_type":"stream"}]},{"cell_type":"code","source":"train_dataset, valid_dataset = dataset.split(\n    split_ratio = 0.6,\n    stratified =True,\n    strata_field=\"target\"\n)\nlen(train_dataset), len(valid_dataset)","metadata":{"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"(4568, 3045)"},"metadata":{}}]},{"cell_type":"code","source":"# Hacemos el vocabulario de las etiquetas, 0 o 1\nLABEL.build_vocab(train_dataset)\nlen(LABEL.vocab)","metadata":{"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"2"},"metadata":{}}]},{"cell_type":"code","source":"BATCH_SIZE = 64\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndataloader = {\n    \"train\": torchtext.data.BucketIterator(train_dataset, batch_size=BATCH_SIZE, shuffle=True, device=DEVICE),\n    \"val\":  torchtext.data.BucketIterator(valid_dataset, batch_size=200, device=DEVICE) # Ponemos un mayor batch_size porque no tenemos que calcular gradientes y podremos ir mas rapido\n    \n}","metadata":{"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"## Modelo de LSTM cargando embeddings","metadata":{}},{"cell_type":"code","source":"# CONSTRUIMOS RED NEURONAL\nclass BERT(torch.nn.Module):\n    def __init__(self, hidden_size=128, num_layers=2, n_outputs=2, bidirectional=False, dropout=0):\n        super().__init__()\n        self.bert= BertModel.from_pretrained(\"bert-base-uncased\") # Calculará los embeding\n        \n        for name, param in self.bert.named_parameters(): # Le dice a todas capas de Bert que no aplique gradientes\n            param.requires_grad=False\n        \n        self.rnn = torch.nn.LSTM( # Cargamos una red LSTM\n            input_size= self.bert.config.to_dict()[\"hidden_size\"], # la funcion config nos permite sacar las dimensiones de la capa oculta para pasarsela a nuestro modelo\n            hidden_size = hidden_size,\n            num_layers=num_layers,\n            bidirectional=bidirectional,\n            dropout= dropout,\n            batch_first=True\n        )\n        self.fc = torch.nn.Linear(2*hidden_size if bidirectional else hidden_size, n_outputs)\n    \n    def forward(self, text):\n        with torch.no_grad(): # Que no calcule gradientes \n            embedded = self.bert(text)[0]\n        output, _ = self.rnn(embedded) # La capa de RNN nos devolvera los outputs y el valor del ultimo hidden state que no lo queremos para nada\n        # Dimensiones en RNN y torchtext son [\"batch_size\",\"longitud del texto\",\"hidden_size\"]\n        return self.fc(output[:,-1,:].squeeze(1)) # output[:,-1,:] Nos quedaremos con la ultima palabra, todo el batch_size y el hidden_size osea tenemos [1,64,128] dimensiones luego se lo pasaremos a la capa lineal quitandole la ultima palabra con la funcion squeeze(0)       ","metadata":{"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"# CREAMOS LA FUNCION PARA ENTRENAR NUESTRO MODELO\n\ndef fit(model, dataloader, epochs = 10, lr=1e-3):\n    \n    model.to(DEVICE) #Mandamos nuestro modelo a la gpu\n    criterion = torch.nn.CrossEntropyLoss() # Funcion de perdida CrossEntropyLoss por que queremos mas de una salida sino seria BCE\n    optimizer = torch.optim.Adam(model.parameters(), lr) # Optimizador con los parametros del modelo y añadimos nuestra lr\n    \n    hist = {\"loss\" : [], \"f1\": [], \"val_loss\":[],\"val_f1\":[]} # Guardaremos las metricas\n    best_f1 = 0.\n    for e in range(1, epochs+1):\n        \n        # ENTRENAMOS\n        model.train() # Ponemos el model en modo entrenamiento\n        l, f1s = [], []\n        bar = tqdm(dataloader[\"train\"])\n        for batch in bar:\n            optimizer.zero_grad() # Ponemos a cero los gradientes\n            y_pred = model(batch.text) # Calculamos salida pasando al modelo el texto\n            loss = criterion(y_pred, batch.target) # calulo loss functions\n            l.append(loss.item()) # guardamos loss function\n            loss.backward()#calculo gradiente\n            optimizer.step()# actualizo los pesos\n            y_pred= torch.argmax(y_pred, axis=1)# cogera el indice del valor mas grande, es decir dará 0 o 1\n            f1s.append(m.f1_score(batch.target.cpu(), y_pred.cpu()))\n            bar.set_description(f\"loss {np.mean(l):.5f} f1 {np.mean(f1s):.5f}\")\n        hist[\"loss\"].append(np.mean(l))\n        hist[\"f1\"].append(np.mean(f1s))\n        \n        #EVALUAMOS\n        model.eval()\n        l,acc,f1s = [],[],[]\n        with torch.no_grad():\n            pg_bar = tqdm(dataloader[\"val\"])\n            for batch in pg_bar:\n                y_pred = model(batch.text)\n                loss = criterion (y_pred, batch.target)\n                l.append(loss.item())\n                y_pred = torch.argmax(y_pred, axis=1)\n                f1s.append(m.f1_score(batch.target.cpu(), y_pred.cpu())) # Si trabajas con el paquete sklearn los datos deben estar en la cpu y en formato numpy\n                bar.set_description(f\"val_loss {np.mean(l):.5f} val_f1 {np.mean(f1s):.5f}\")\n        hist[\"val_loss\"].append(np.mean(l))\n        hist[\"val_f1\"].append(np.mean(f1s))\n        # CALLBACKS SAVE BEST MODEL\n        if hist[\"val_f1\"][-1] > best_f1:\n            best_f1 = hist[\"val_f1\"][-1]\n            torch.save(model.state_dict(),\"ckpt.pt\")\n        print(f'Epoch {e}/{epochs} loss:{hist[\"loss\"][-1]:.5f} f1:{hist[\"f1\"][-1]:.5f} val_loss:{hist[\"val_loss\"][-1]:.5f} val_f1:{hist[\"val_f1\"][-1]:.5f}')\n    model.load_state_dict(torch.load(\"ckpt.pt\"))\n    return hist","metadata":{"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"# INSTANCIAMOS LA RNN Y ENTRENAMOS\nmodel = BERT()\n\n\nhist = fit(model, dataloader)","metadata":{"trusted":true},"execution_count":35,"outputs":[{"name":"stderr","text":"loss 0.65698 f1 0.41948: 100%|██████████| 72/72 [00:09<00:00,  7.43it/s]\n100%|██████████| 16/16 [00:05<00:00,  2.77it/s]\nloss 0.66474 f1 0.64516:   1%|▏         | 1/72 [00:00<00:09,  7.24it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch 1/10 loss:0.65698 f1:0.41948 val_loss:0.65019 val_f1:0.62051\n","output_type":"stream"},{"name":"stderr","text":"loss 0.64062 f1 0.51245: 100%|██████████| 72/72 [00:09<00:00,  7.48it/s]\n100%|██████████| 16/16 [00:05<00:00,  2.81it/s]\nloss 0.62387 f1 0.08000:   1%|▏         | 1/72 [00:00<00:11,  6.03it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch 2/10 loss:0.64062 f1:0.51245 val_loss:0.64518 val_f1:0.05085\n","output_type":"stream"},{"name":"stderr","text":"loss 0.64984 f1 0.43493: 100%|██████████| 72/72 [00:09<00:00,  7.45it/s]\n100%|██████████| 16/16 [00:05<00:00,  2.79it/s]\nloss 0.67000 f1 0.64000:   1%|▏         | 1/72 [00:00<00:10,  6.57it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch 3/10 loss:0.64984 f1:0.43493 val_loss:0.65432 val_f1:0.66300\n","output_type":"stream"},{"name":"stderr","text":"loss 0.57899 f1 0.61449: 100%|██████████| 72/72 [00:09<00:00,  7.48it/s]\n100%|██████████| 16/16 [00:05<00:00,  2.82it/s]\nloss 0.48948 f1 0.71538:   1%|▏         | 1/72 [00:00<00:09,  7.37it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch 4/10 loss:0.57899 f1:0.61449 val_loss:0.47913 val_f1:0.76876\n","output_type":"stream"},{"name":"stderr","text":"loss 0.46804 f1 0.75475: 100%|██████████| 72/72 [00:09<00:00,  7.54it/s]\n100%|██████████| 16/16 [00:05<00:00,  2.77it/s]\nloss 0.55805 f1 0.72727:   1%|▏         | 1/72 [00:00<00:10,  6.57it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch 5/10 loss:0.46804 f1:0.75475 val_loss:0.48693 val_f1:0.77867\n","output_type":"stream"},{"name":"stderr","text":"loss 0.43698 f1 0.76622: 100%|██████████| 72/72 [00:09<00:00,  7.46it/s]\n100%|██████████| 16/16 [00:05<00:00,  2.79it/s]\nloss 0.41806 f1 0.71111:   1%|▏         | 1/72 [00:00<00:09,  7.16it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch 6/10 loss:0.43698 f1:0.76622 val_loss:0.41340 val_f1:0.77507\n","output_type":"stream"},{"name":"stderr","text":"loss 0.40911 f1 0.77729: 100%|██████████| 72/72 [00:09<00:00,  7.52it/s]\n100%|██████████| 16/16 [00:05<00:00,  2.81it/s]\nloss 0.34006 f1 0.85185:   1%|▏         | 1/72 [00:00<00:13,  5.38it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch 7/10 loss:0.40911 f1:0.77729 val_loss:0.41123 val_f1:0.78341\n","output_type":"stream"},{"name":"stderr","text":"loss 0.38647 f1 0.80216: 100%|██████████| 72/72 [00:09<00:00,  7.48it/s]\n100%|██████████| 16/16 [00:05<00:00,  2.80it/s]\nloss 0.35312 f1 0.75556:   1%|▏         | 1/72 [00:00<00:10,  6.96it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch 8/10 loss:0.38647 f1:0.80216 val_loss:0.40232 val_f1:0.77110\n","output_type":"stream"},{"name":"stderr","text":"loss 0.38150 f1 0.80011: 100%|██████████| 72/72 [00:09<00:00,  7.46it/s]\n100%|██████████| 16/16 [00:05<00:00,  2.79it/s]\nloss 0.33675 f1 0.76190:   1%|▏         | 1/72 [00:00<00:10,  6.58it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch 9/10 loss:0.38150 f1:0.80011 val_loss:0.40543 val_f1:0.78831\n","output_type":"stream"},{"name":"stderr","text":"loss 0.36471 f1 0.80490: 100%|██████████| 72/72 [00:09<00:00,  7.47it/s]\n100%|██████████| 16/16 [00:05<00:00,  2.74it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10/10 loss:0.36471 f1:0.80490 val_loss:0.41764 val_f1:0.78997\n","output_type":"stream"}]},{"cell_type":"code","source":"def plot(hist): # Funcion para graficar nuestras metricas \n    fig = plt.figure(dpi = 200, figsize= (10,3))\n    ax = plt.subplot(121)\n    hist = pd.DataFrame(hist)\n    hist[[\"loss\",\"val_loss\"]].plot(ax=ax, grid=True)\n    ax = plt.subplot(122)\n    hist[[\"f1\", \"val_f1\"]].plot(ax=ax, grid=True)\n    plt.show()\nplot(hist)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Estos malos resultados es porque las RNN sencillas no funcionan bien con longuitudes grandes. Van bien con secuancias de texto de 10-20 pero en nuestro caso podemos tener unas secuencias mayor a 100, entonces estas redes tan sencillas fallan mucho. Deberemos buscar mejores modelos como LSTM, bidireccionales o transformers","metadata":{}},{"cell_type":"markdown","source":"## TEST","metadata":{}},{"cell_type":"code","source":"# Creamos nuestros dataset de test\ntest_dataset = torchtext.data.TabularDataset(\n    path=PATH/'test.csv',\n    format = \"CSV\",\n    fields = [(\"id\",ID),(\"keyword\",KEYWORD),(\"location\",LOCATION),(\"text\",TEXT)],\n    skip_header=True\n)\nlen(test_dataset)","metadata":{"trusted":true},"execution_count":36,"outputs":[{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"3263"},"metadata":{}}]},{"cell_type":"code","source":"# Comprobamos si es testo correcto\nix=3258\nprint(vars(test_dataset.examples[ix]))","metadata":{"trusted":true},"execution_count":37,"outputs":[{"name":"stdout","text":"{'id': '10861', 'keyword': '', 'location': '', 'text': [8372, 3808, 3050, 3349, 1057, 2080, 3808, 3435, 24454, 2015, 1060, 2099, 7962]}\n","output_type":"stream"}]},{"cell_type":"code","source":"test_dataloader = torchtext.data.BucketIterator(test_dataset, batch_size=BATCH_SIZE, shuffle=False, device=DEVICE)","metadata":{"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"def predict():\n    model.eval()\n    preds = torch.tensor([]).to(DEVICE)\n    with torch.no_grad():\n        for batch in tqdm(test_dataloader):\n            y_pred=model(batch.text)\n            y_pred = torch.argmax(y_pred, axis=1)\n            preds = torch.cat([preds, y_pred])\n    return preds","metadata":{"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"preds = predict()\npreds","metadata":{"trusted":true},"execution_count":41,"outputs":[{"name":"stderr","text":"100%|██████████| 51/51 [00:05<00:00,  8.59it/s]\n","output_type":"stream"},{"execution_count":41,"output_type":"execute_result","data":{"text/plain":"tensor([1., 1., 1.,  ..., 1., 1., 1.], device='cuda:0')"},"metadata":{}}]},{"cell_type":"code","source":"submission = pd.read_csv(\"/kaggle/input/nlp-getting-started/sample_submission.csv\")\nsubmission.target = preds.cpu().long()# Me traigo mis pres a la cpu \nsubmission.to_csv(\"submission.csv\", index=False)","metadata":{"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}