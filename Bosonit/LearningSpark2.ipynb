{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Spark\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capítulo 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Aplicar no solo count (para obtener el número de líneas) y show sino probar distintas sobrecargas del método show (con/sin truncate, indicando/sin indicar num de filas, etc) así como también los métodos, head, take, first (diferencias entre estos 3?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "quijote = spark.read.text(\"data/el_quijote.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de lineas del archivo el quijote: 2186\n"
     ]
    }
   ],
   "source": [
    "print(f\"Número de lineas del archivo el quijote: {quijote.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|DON QUIJOTE DE LA...|\n",
      "|Miguel de Cervant...|\n",
      "|                    |\n",
      "|       PRIMERA PARTE|\n",
      "|CAPÍTULO 1: Que ...|\n",
      "|En un lugar de la...|\n",
      "|Tuvo muchas veces...|\n",
      "|En resolución, e...|\n",
      "|historia más cie...|\n",
      "|Decía él, que e...|\n",
      "|En efecto, remata...|\n",
      "|Imaginábase el p...|\n",
      "|linaje y patria, ...|\n",
      "|Limpias, pues, su...|\n",
      "|Capítulo 2: Que ...|\n",
      "|Hechas, pues, est...|\n",
      "|Estos pensamiento...|\n",
      "|Con estos iba ens...|\n",
      "|Autores hay que d...|\n",
      "|muertos de hambre...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "quijote.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|DON QUIJOTE DE LA...|\n",
      "|Miguel de Cervant...|\n",
      "|                    |\n",
      "|       PRIMERA PARTE|\n",
      "|CAPÍTULO 1: Que ...|\n",
      "|En un lugar de la...|\n",
      "|Tuvo muchas veces...|\n",
      "|En resolución, e...|\n",
      "|historia más cie...|\n",
      "|Decía él, que e...|\n",
      "+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "quijote.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diferencias entre head, take, first:\n",
    "- Head : De forma predefinida te muestra las 20 primeras lineas\n",
    "- Take : Es necesario introducir un número de líneas\n",
    "- First : Solo te muestra la primera linea, no admite argumentos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Del ejercicio de M&M aplicar: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm_df = spark.read.format(\"csv\").option(\"header\",\"true\").load(\"data/mm.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-----+\n",
      "|State|Color|Count|\n",
      "+-----+-----+-----+\n",
      "|   TX|  Red|   20|\n",
      "|   NV| Blue|   66|\n",
      "+-----+-----+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mm_df.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Otras operaciones de agregación como el Max con otro tipo de ordenamiento (descendiente)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---+---+------------------+\n",
      "|State| Color|Max|Min|             Media|\n",
      "+-----+------+---+---+------------------+\n",
      "|   CO| Brown| 99| 10| 56.57729468599034|\n",
      "|   WY| Brown| 99| 10| 56.20757180156658|\n",
      "|   NM|   Red| 99| 10| 56.03491124260355|\n",
      "|   TX|Orange| 99| 10|55.880750605326874|\n",
      "|   WA|Yellow| 99| 10|  55.8749248346362|\n",
      "|   CA|Yellow| 99| 10|  55.8693967902601|\n",
      "|   WA|   Red| 99| 10| 55.85397965290245|\n",
      "|   NV| Brown| 99| 10| 55.81050090525045|\n",
      "|   WA| Brown| 99| 10| 55.77112043139604|\n",
      "|   CA| Brown| 99| 10|55.740395809080326|\n",
      "|   WY| Green| 99| 10|55.657227138643066|\n",
      "|   CA|  Blue| 99| 10| 55.59762944479102|\n",
      "|   UT| Green| 99| 10|55.557510999371466|\n",
      "|   NV|   Red| 99| 10|  55.4944099378882|\n",
      "|   CO|Orange| 99| 10|55.402557856272836|\n",
      "|   NM| Brown| 99| 10|55.392412566686424|\n",
      "|   WA|  Blue| 99| 10|55.314461538461536|\n",
      "|   TX|   Red| 99| 10|55.306666666666665|\n",
      "|   TX| Brown| 99| 10| 55.29311395490554|\n",
      "|   CA|   Red| 99| 10| 55.26992753623188|\n",
      "+-----+------+---+---+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "mm_df.groupby(\"State\",\"Color\").agg(max(\"Count\").alias(\"Max\"),min(\"Count\").alias(\"Min\"),avg(\"Count\").alias(\"Media\")).orderBy(desc(\"Media\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hacer un ejercicio como el “where” de CA que aparece en el libro pero indicando más opciones de estados (p.e. NV, TX, CA, CO)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---+---+------------------+\n",
      "|State| Color|Max|Min|             Media|\n",
      "+-----+------+---+---+------------------+\n",
      "|   TX|  Blue| 99| 10|54.811648079306075|\n",
      "|   TX|   Red| 99| 10|55.306666666666665|\n",
      "|   TX| Brown| 99| 10| 55.29311395490554|\n",
      "|   TX|Yellow| 99| 10| 55.09042865531415|\n",
      "|   TX|Orange| 99| 10|55.880750605326874|\n",
      "|   TX| Green| 99| 10| 55.12550374208405|\n",
      "|   NV|Orange| 99| 10|54.865070093457945|\n",
      "|   NV| Brown| 99| 10| 55.81050090525045|\n",
      "|   NV|   Red| 99| 10|  55.4944099378882|\n",
      "|   NV| Green| 99| 10| 53.78739693757362|\n",
      "|   NV|Yellow| 99| 10|54.561194029850746|\n",
      "|   NV|  Blue| 99| 10|53.797369994022716|\n",
      "|   CO| Brown| 99| 10| 56.57729468599034|\n",
      "|   CO|  Blue| 99| 10| 55.11032448377581|\n",
      "|   CO|   Red| 99| 10|55.089285714285715|\n",
      "|   CO| Green| 99| 10| 54.71336835960304|\n",
      "|   CO|Yellow| 99| 10| 55.22254503195816|\n",
      "|   CO|Orange| 99| 10|55.402557856272836|\n",
      "|   CA|  Blue| 99| 10| 55.59762944479102|\n",
      "|   CA|Yellow| 99| 10|  55.8693967902601|\n",
      "|   CA|   Red| 99| 10| 55.26992753623188|\n",
      "|   CA| Brown| 99| 10|55.740395809080326|\n",
      "|   CA|Orange| 99| 10|54.502715751357876|\n",
      "|   CA| Green| 99| 10|54.268717353453276|\n",
      "+-----+------+---+---+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mm_df.where(col(\"State\").isin([\"NV\",\"TX\",\"CA\",\"CO\"])).groupby(\"State\",\"Color\").agg(max(\"Count\").alias(\"Max\"),min(\"Count\").alias(\"Min\"),avg(\"Count\").alias(\"Media\")).orderBy(desc(\"State\")).show(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------+\n",
      "| Color|             Media|\n",
      "+------+------------------+\n",
      "|Orange| 54.84775708211056|\n",
      "| Green|54.711129489603024|\n",
      "|  Blue| 54.86752994102985|\n",
      "| Brown|55.463537250151425|\n",
      "|Yellow| 54.95409621338414|\n",
      "|   Red| 55.16962512786569|\n",
      "+------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mm_df.createOrReplaceTempView(\"mm\")\n",
    "spark.sql(\"SELECT Color, avg(Count) as Media FROM mm GROUP BY Color\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-----------+\n",
      "|database|tableName|isTemporary|\n",
      "+--------+---------+-----------+\n",
      "|        |       mm|       true|\n",
      "+--------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW TABLES\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- State: string (nullable = true)\n",
      " |-- Color: string (nullable = true)\n",
      " |-- Count: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mm_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capitulo 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Leer el CSV del ejemplo del cap2 y obtener la estructura del schema dado por defecto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm_df = spark.read.format(\"csv\").option(\"header\",\"true\").option(\"inferSchema\",\"true\").load(\"data/mm.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- State: string (nullable = true)\n",
      " |-- Color: string (nullable = true)\n",
      " |-- Count: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mm_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Cuando se define un schema al definir un campo por ejemplo StructField('Delay', FloatType(), True) ¿qué significa el último parámetro Boolean?\n",
    "\n",
    "El tercer argumento nos permite la posibilidad de que se permita introducir valores nulos en el campo. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Dataset vs DataFrame (Scala). ¿En qué se diferencian a nivel de código?\n",
    "\n",
    "Una de la diferencias entre los Datasets y los DataFrame son los tipados en los campos. Los Datasets tiene un fuerte tipado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.Utilizando el mismo ejemplo utilizado en el capítulo para guardar en parquet y guardar los datos en los formatos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "fire_call = spark.read.format(\"csv\").option(\"header\",\"true\").option(\"inferSchema\",\"true\").load(\"data/sf-fire-calls.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save formato parquet\n",
    "fire_call.write.parquet(\"data/fire_call.parquet\")\n",
    "# Save formato avro\n",
    "fire_call.write.format(\"avro\").save(\"data/fire_call.avro\")\n",
    "# Save formato csv\n",
    "fire_call.write.csv(\"data/fire_call.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capitulo 4 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. GlobalTempView vs TempView\n",
    "\n",
    "La diferencia es la accesibilidad, mediante el comando GlobalTempView puedes acceder a la view en las diferentes sessiones activas mientras que con las Tempview no."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.Leer los AVRO, Parquet, JSON y CSV escritos en el cap3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load formato parquet\n",
    "fire_parquet = spark.read.parquet(\"data/fire_call.parquet\")\n",
    "# Load formato avro\n",
    "fire_avro = spark.read.format(\"avro\").load(\"data/fire_call.avro\")\n",
    "# Load formato csv\n",
    "fire_csv = spark.read.format(\"csv\").load(\"data/fire_call.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capitulo 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Pros y Cons utilizar UDFs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nos permite serializar las funciones del lenguaje python para poder utilizarlas en Spark pero tiene problemas de rendimiento comparado con Spark SQL ya que debe mandar la informacion de los JVM a Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.Instalar MySQL, descargar driver y cargar datos de BBDD de empleados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar con spark datos de empleados, departamentos y salario\n",
    "empleados = spark.read\\\n",
    "                .format(\"jdbc\")\\\n",
    "                .option(\"url\", \"jdbc:mysql://localhost:3306/employees\")\\\n",
    "                .option(\"driver\", \"com.mysql.jdbc.Driver\")\\\n",
    "                .option(\"dbtable\", \"employees\")\\\n",
    "                .option(\"user\", \"root\")\\\n",
    "                .option(\"password\", \"lutecio\")\\\n",
    "                .load()\n",
    "departamentos = spark.read\\\n",
    "                .format(\"jdbc\")\\\n",
    "                .option(\"url\", \"jdbc:mysql://localhost:3306/employees\")\\\n",
    "                .option(\"driver\", \"com.mysql.jdbc.Driver\")\\\n",
    "                .option(\"dbtable\", \"departments\")\\\n",
    "                .option(\"user\", \"root\")\\\n",
    "                .option(\"password\", \"lutecio\")\\\n",
    "                .load()\n",
    "\n",
    "salario = spark.read\\\n",
    "                .format(\"jdbc\")\\\n",
    "                .option(\"url\", \"jdbc:mysql://localhost:3306/employees\")\\\n",
    "                .option(\"driver\", \"com.mysql.jdbc.Driver\")\\\n",
    "                .option(\"dbtable\", \"salaries\")\\\n",
    "                .option(\"user\", \"root\")\\\n",
    "                .option(\"password\", \"lutecio\")\\\n",
    "                .load()\n",
    "titulo = spark.read\\\n",
    "                .format(\"jdbc\")\\\n",
    "                .option(\"url\", \"jdbc:mysql://localhost:3306/employees\")\\\n",
    "                .option(\"driver\", \"com.mysql.jdbc.Driver\")\\\n",
    "                .option(\"dbtable\", \"titles\")\\\n",
    "                .option(\"user\", \"root\")\\\n",
    "                .option(\"password\", \"lutecio\")\\\n",
    "                .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------+---------+------+----------+------+------+----------+----------+------+----------------+----------+----------+\n",
      "|emp_no|birth_date|first_name|last_name|gender| hire_date|emp_no|salary| from_date|   to_date|emp_no|           title| from_date|   to_date|\n",
      "+------+----------+----------+---------+------+----------+------+------+----------+----------+------+----------------+----------+----------+\n",
      "| 10206|1960-09-19|  Alassane|  Iwayama|     F|1988-04-19| 10206| 40000|1988-04-19|1989-04-19| 10206|Technique Leader|1988-04-19|9999-01-01|\n",
      "| 10206|1960-09-19|  Alassane|  Iwayama|     F|1988-04-19| 10206| 43519|1989-04-19|1990-04-19| 10206|Technique Leader|1988-04-19|9999-01-01|\n",
      "| 10206|1960-09-19|  Alassane|  Iwayama|     F|1988-04-19| 10206| 46265|1990-04-19|1991-04-19| 10206|Technique Leader|1988-04-19|9999-01-01|\n",
      "| 10206|1960-09-19|  Alassane|  Iwayama|     F|1988-04-19| 10206| 46865|1991-04-19|1992-04-18| 10206|Technique Leader|1988-04-19|9999-01-01|\n",
      "| 10206|1960-09-19|  Alassane|  Iwayama|     F|1988-04-19| 10206| 47837|1992-04-18|1993-04-18| 10206|Technique Leader|1988-04-19|9999-01-01|\n",
      "| 10206|1960-09-19|  Alassane|  Iwayama|     F|1988-04-19| 10206| 52042|1993-04-18|1994-04-18| 10206|Technique Leader|1988-04-19|9999-01-01|\n",
      "| 10206|1960-09-19|  Alassane|  Iwayama|     F|1988-04-19| 10206| 52370|1994-04-18|1995-04-18| 10206|Technique Leader|1988-04-19|9999-01-01|\n",
      "| 10206|1960-09-19|  Alassane|  Iwayama|     F|1988-04-19| 10206| 53202|1995-04-18|1996-04-17| 10206|Technique Leader|1988-04-19|9999-01-01|\n",
      "| 10206|1960-09-19|  Alassane|  Iwayama|     F|1988-04-19| 10206| 56087|1996-04-17|1997-04-17| 10206|Technique Leader|1988-04-19|9999-01-01|\n",
      "| 10206|1960-09-19|  Alassane|  Iwayama|     F|1988-04-19| 10206| 59252|1997-04-17|1998-04-17| 10206|Technique Leader|1988-04-19|9999-01-01|\n",
      "| 10206|1960-09-19|  Alassane|  Iwayama|     F|1988-04-19| 10206| 62716|1998-04-17|1999-04-17| 10206|Technique Leader|1988-04-19|9999-01-01|\n",
      "| 10206|1960-09-19|  Alassane|  Iwayama|     F|1988-04-19| 10206| 67137|1999-04-17|2000-04-16| 10206|Technique Leader|1988-04-19|9999-01-01|\n",
      "| 10206|1960-09-19|  Alassane|  Iwayama|     F|1988-04-19| 10206| 67944|2000-04-16|2001-04-16| 10206|Technique Leader|1988-04-19|9999-01-01|\n",
      "| 10206|1960-09-19|  Alassane|  Iwayama|     F|1988-04-19| 10206| 67588|2001-04-16|2002-04-16| 10206|Technique Leader|1988-04-19|9999-01-01|\n",
      "| 10206|1960-09-19|  Alassane|  Iwayama|     F|1988-04-19| 10206| 71052|2002-04-16|9999-01-01| 10206|Technique Leader|1988-04-19|9999-01-01|\n",
      "| 10362|1963-09-16|   Shalesh|  dAstous|     M|1988-08-24| 10362| 40000|1990-11-02|1991-11-02| 10362|    Senior Staff|1990-11-02|1997-07-16|\n",
      "| 10362|1963-09-16|   Shalesh|  dAstous|     M|1988-08-24| 10362| 44091|1991-11-02|1992-11-01| 10362|    Senior Staff|1990-11-02|1997-07-16|\n",
      "| 10362|1963-09-16|   Shalesh|  dAstous|     M|1988-08-24| 10362| 45546|1992-11-01|1993-11-01| 10362|    Senior Staff|1990-11-02|1997-07-16|\n",
      "| 10362|1963-09-16|   Shalesh|  dAstous|     M|1988-08-24| 10362| 49296|1993-11-01|1994-11-01| 10362|    Senior Staff|1990-11-02|1997-07-16|\n",
      "| 10362|1963-09-16|   Shalesh|  dAstous|     M|1988-08-24| 10362| 48889|1994-11-01|1995-11-01| 10362|    Senior Staff|1990-11-02|1997-07-16|\n",
      "+------+----------+----------+---------+------+----------+------+------+----------+----------+------+----------------+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empleados.join(salario, empleados.emp_no ==salario.emp_no).join(titulo,empleados.emp_no==titulo.emp_no).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.Diferencia entre Rank y dense_rank (operaciones de ventana)\n",
    "Ambas funciones sirven para clasificar un registro dentro de un grupo de filas, la diferencia que en caso de que dos registros tengan la misma posición con la función Dense_rank saltara los puestos para el siguiente."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
