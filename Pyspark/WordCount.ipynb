{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Spark Logo](http://spark-mooc.github.io/web-assets/images/ta_Spark-logo-small.png)  ![Python Logo](http://spark-mooc.github.io/web-assets/images/python-logo-master-v3-TM-flattened_small.png)\n",
    "# Contando palabras: Construye una aplicacion que cuente palabras de forma eficiente\n",
    "\n",
    "Este laboratorio usara las tecnologias descritas en los materiales del curso sobre Spark para desarrollar una aplicacion de conteo de palabras. \n",
    "\n",
    "Con el uso masivo de Internet y las redes sociales, el volumen de texto no estructurado esta creciendo dramaticamente, y Spark es una gran herramienta para analizar este tipo de datos. En esta PEC, vamos a escribir codigo para encontrar las palabras mas comunes en un texto generado en latin, el ya conocido [Lorem Ipsum](https://www.lipsum.com/).\n",
    "\n",
    "\n",
    "Lo mas interesante de la forma de trabajar en esta practica es que podria escalarse para, por ejemplo, encontrar las palabras mas comunes en Wikipedia.\n",
    "\n",
    "## Durante esta PEC vamos a cubrir:\n",
    "\n",
    "* *Parte 1:* Creación de un RDD y un pair RDD\n",
    "* *Parte 2:* Contar palabras usando un pair RDD\n",
    "* *Parte 3:* Encontrar las palabras individuales y su frecuencia de aparicion media\n",
    "* *Parte 4:* Aplicar las funcionalidades desarrolladas a un archivo de texto* \n",
    "* *Parte 5:* Calcular algunos estadisticos*\n",
    "\n",
    "\n",
    "> Como referencia a todos los detalles de los metodos que se usan en esta practica usar:\n",
    "> * [API Python de Spark](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 1: Creacion de un RDD y un pair RDDs\n",
    "\n",
    "En esta seccion, exploraremos como crear RRDs usando `parallelize` y como aplicar pair RDDs al problema del conteo de palabras.\n",
    "\n",
    "### (0) Configuración del entorno python + spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "Exception",
     "evalue": "Java gateway process exited before sending its port number",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-5dce109035f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaster\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"local[*]\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappName\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Gonzalezjulvez\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/Projects/engine/lib/python3.8/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    131\u001b[0m                 \" is not allowed as it is a security risk.\")\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[0;32m~/Documents/Projects/engine/lib/python3.8/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m                 \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgateway\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlaunch_gateway\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m                 \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjvm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Projects/engine/lib/python3.8/site-packages/pyspark/java_gateway.py\u001b[0m in \u001b[0;36mlaunch_gateway\u001b[0;34m(conf, popen_kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn_info_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Java gateway process exited before sending its port number\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn_info_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: Java gateway process exited before sending its port number"
     ]
    }
   ],
   "source": [
    "#import findspark\n",
    "# findspark.init()\n",
    "import pyspark\n",
    "import random\n",
    "sc = pyspark.SparkContext(master=\"local\", appName=\"Gonzalezjulvez\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1a) Creación de un RDD\n",
    "Empezemos generando un RDD a partir de una lista de Python y el metodo `sc.parallelize`. Luego mostraremos por pantalla el tipo de la variable generada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordsList = ['cat', 'elephant', 'rat', 'rat', 'cat']\n",
    "wordsRDD = sc.parallelize(wordsList, 4)\n",
    "# Print out the type of wordsRDD\n",
    "type(wordsRDD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1b) Crear el plural de las palabras y testear\n",
    "\n",
    "Vamos a utilizar una transformacion `map()` para incorporar la letra 's' a cada uno de los strings almacenados en el RDD que acabamos de crear. Vamos a definir una funcion de Python que devuelva una palabra, que se le ha pasado como parametro, incorporando una \"s\" al final de la misma. Reemplazar el texto `<FILL IN>` con la solucion propuesta. Despues de haber definido correctamente la funcion `makePlural`, ejecutar la segunda celda que contiene un assert de test. Si la solucion es correcta, se imprimira `1 test passed`.\n",
    "\n",
    "Esta sera la forma habitual de trabajar en las PECs. Los ejercicios contendran una explicacion de lo que se espera, seguido de una celda de codigo con uno o mas `<FILL IN>`. Las celdas que necesiten ser modificadas contendran el texto `# TODO: Replace <FILL IN> with appropriate code` en la primera linea.\n",
    "\n",
    "Una vez se hayan sustituido todos los `<FILL IN>` por el codigo Python adecuado, ejecutar la celda, y posteriormente ejecutar la celda siguiente de test para comprobar que que la solucion es la esperada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cats'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "def makePlural(word):\n",
    "    \"\"\"Adds an 's' to `word`.\n",
    "\n",
    "    Note:\n",
    "        This is a simple function that only adds an 's'.  \n",
    "\n",
    "    Args:\n",
    "        word (str): A string.\n",
    "\n",
    "    Returns:\n",
    "        str: A string with 's' added to it.\n",
    "    \"\"\"\n",
    "    \n",
    "    return word + 's'\n",
    "\n",
    "makePlural('cat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST Pluralize and test (1b)\n",
    "assert makePlural('rat') == 'rats', 'incorrect result: makePlural does not add an s' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1c) Aplicar `makePlural` a nuestro RDD\n",
    "\n",
    "Ahora es el momento de aplicar nuestra funcion `makePlural()` a todos los elementos del RDD usando una transformacion [map()](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.map). Posteriormente ejecutar la accion [collect()](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.collect) para obtener el RDD transformado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cats', 'elephants', 'rats', 'rats', 'cats']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "pluralRDD = wordsRDD.map(makePlural)\n",
    "pluralRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST Apply makePlural to the base RDD(1c)\n",
    "assert pluralRDD.collect() == ['cats', 'elephants', 'rats', 'rats', 'cats'], 'incorrect values for pluralRDD'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1d) Ejecutar una funcion `lambda` en un `map`\n",
    "\n",
    "Vamos a crear el mismo RDD usando una `lambda` function en lugar de una funcion con nombre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cats', 'elephants', 'rats', 'rats', 'cats']\n"
     ]
    }
   ],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "pluralLambdaRDD = wordsRDD.map(lambda word : makePlural(word))\n",
    "print(pluralLambdaRDD.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST Pass a lambda function to map (1d)\n",
    "assert pluralLambdaRDD.collect() == ['cats', 'elephants', 'rats', 'rats', 'cats'], 'incorrect values for pluralLambdaRDD (1d)'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1e) Numero de caracteres de cada una de las palabras\n",
    "\n",
    "Ahora vamos a usar un `map()` y una funcion lambda `lambda` para obtener el numero de caracteres de cada palabra. Usaremos `collect` para guardar este resultado directamente en una variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 9, 4, 4, 4]\n"
     ]
    }
   ],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "pluralLengths = (pluralRDD.map(lambda word : len(word)).collect())\n",
    "print(pluralLengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST Length of each word (1e)\n",
    "assert pluralLengths == [4, 9, 4, 4, 4], 'incorrect values for pluralLengths'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1f) Pair RDDs\n",
    "\n",
    "El siguiente paso para completar nuestro programa de conteo de palabras en crear un nuevo tipo de RDD, llamado pair RDD. Un pair RDD es un RDD donde cada elemento es un tupla del estilo `(k, v)` donde `k` es la clave y `v` es su valor correspondiente. En este ejemplo, crearemos una pair RDD consistente en tuplas con el formato `('<word>', 1)` para cada elemento de nuestro RDD basico.\n",
    "\n",
    "Podemos crear nuestro pair RDD usando una transformacion `map()` con una `lambda()` function que cree un nuevo RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('cat', 1), ('elephant', 1), ('rat', 1), ('rat', 1), ('cat', 1)]\n"
     ]
    }
   ],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "wordPairs = wordsRDD.map(lambda word : (word,1))\n",
    "print(wordPairs.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST Pair RDDs (1f)\n",
    "assert wordPairs.collect() == [('cat', 1), ('elephant', 1), ('rat', 1), ('rat', 1), ('cat', 1)], 'incorrect value for wordPairs'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 2: Contar palabras usando un pair RDD\n",
    "\n",
    "Ahora, contaremos el numero de veces que una palabra en particular aparece en el RDD. Esta operacion se puede realizar de una infinidad de maneras, pero algunas seran mucho menos eficientes que otras.\n",
    "\n",
    "Un solucion muy sencilla seria usar `collect()` sobre todos los elementos devolverlos al driver y alli contarlos. Mientras esta forma de trabajar podria funcionar con textos relativamente cortos, nosotros lo que queremos es poder trabajar con textos de cualquier longitud. Adicionalmente, ejecutar todo el calculo en el driver es mucho mas lento que ejecutarlo en paralelo en los workers. Por estos motivos, en esta practica usaremos operaciones paralelizables.\n",
    "\n",
    "%md\n",
    "### (2a) Usando `groupByKey()`\n",
    "Una primera solucion a nuestro problema, luego veremos que hay otras mucho mas eficientes, se podria basar en la transformacion [groupByKey()](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.groupByKey). Como su nombre indica, la transformacion `groupByKey()` agrupa todos los elementos de un RDD que compartan la misma clave en una unica lista dentro de una de las particiones.\n",
    "\n",
    "Esta operacion plantea dos problemas:\n",
    "  + Esta operacion necesita mover todos los valores dentro de la particion adecuada. Esto satura la red. \n",
    "  + Las listas generadas pueden llegar a ser muy grandes llegando incluso a saturar la memoria de alguno de los trabajadadores\n",
    "  \n",
    "Utiliza `groupByKey()` para generar un pair RDD del tipo `('word', iterator)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat: [1, 1]\n",
      "elephant: [1]\n",
      "rat: [1, 1]\n"
     ]
    }
   ],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "# Note that groupByKey requires no parameters\n",
    "wordsGrouped = wordPairs.groupByKey()\n",
    "for key, value in wordsGrouped.collect():\n",
    "    print('{0}: {1}'.format(key, list(value)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST groupByKey() approach (2a)\n",
    "assert sorted(wordsGrouped.mapValues(lambda x: list(x)).collect()) == [('cat', [1, 1]), ('elephant', [1]), ('rat', [1, 1])], 'incorrect value for wordsGrouped'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2b) Utiliza `groupByKey()` para obtener los conteos\n",
    "\n",
    "Usando la transformacion `groupByKey()` crea un RDD que contenga 2 elementos, donde cada uno de ellos sea un par palabra (clave) iterador de Python (valor).\n",
    "\n",
    "Luego suma todos los valores de iterador usando una transformacion `map()`. El resultado debe ser un pair RDD que contenga las parejas (word, count)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('cat', 2), ('elephant', 1), ('rat', 2)]\n"
     ]
    }
   ],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "wordCountsGrouped = wordsGrouped.map(lambda x : (x[0], len(x[1])))\n",
    "print(wordCountsGrouped.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST Use groupByKey() to obtain the counts (2b)\n",
    "assert sorted(wordCountsGrouped.collect())==[('cat', 2), ('elephant', 1), ('rat', 2)],'incorrect value for wordCountsGrouped'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2c) Conteo usando `reduceByKey`\n",
    "\n",
    "Una mejor solucion es comenzar desde un pair RDD y luego usar la transformacion [reduceByKey()](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.reduceByKey) para crear un nuevo pair RDD. La transformacion `reduceByKey()` agrupa todas las parejas que comparten la misma clave. Posteriormente aplica la funcion que se le pasa por parametro agrupando los valores de dos en dos. Este proceso se repite iterativamente hasta que obtenemos un unico valor agregado para cada una de las claves del pair RDD. `reduceByKey()` opera aplicando la funcion primero dentro de cada una de las particiones de forma independiente, y posteriormente unicamente comparte los valores agregados entre particiones diferentes, permitiendole escalar de forma eficiente ya que no tiene necesidad de desplazar por la red una gran cantidad de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('cat', 2), ('elephant', 1), ('rat', 2)]\n"
     ]
    }
   ],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "# Note that reduceByKey takes in a function that accepts two values and returns a single value\n",
    "wordCounts = wordPairs.reduceByKey(lambda x,y : x+y)\n",
    "print (wordCounts.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST Counting using reduceByKey (2c)\n",
    "assert sorted(wordCounts.collect())==[('cat', 2), ('elephant', 1), ('rat', 2)],'incorrect value for wordCounts'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2d) Ahora todo junto\n",
    "\n",
    "La version mas compleja del codigo ejecuta primero un `map()` sobre el pair RDD, la transformacion `reduceByKey()`, y finalmente la accion `collect()` en una unica linea de codigo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('cat', 2), ('elephant', 1), ('rat', 2)]\n"
     ]
    }
   ],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "wordCountsCollected = (wordsRDD\n",
    "                       .map(lambda word : (word,1))\n",
    "                       .reduceByKey(lambda x,y: x+y)\n",
    "                       .collect())\n",
    "print(wordCountsCollected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST All together (2d)\n",
    "assert sorted(wordCountsCollected)==[('cat', 2), ('elephant', 1), ('rat', 2)],'incorrect value for wordCountsCollected'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 3: Encontrar las palabras individuales y su frecuencia de aparicion media\n",
    "\n",
    "### (3a) Palabras unicas\n",
    "\n",
    "Calcular el numero de palabras unicas en `wordsRDD`. Puedes utitlziar otros RDDs que hayas creado en esta practica si te resulta mas sencillo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "uniqueWords = wordCounts.count()\n",
    "print(uniqueWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST Unique words (3a)\n",
    "assert uniqueWords== 3, 'incorrect count of uniqueWords'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3b) Calular la media usando `reduce()`\n",
    "\n",
    "Encuentra la frequencia media de aparicion de palabras en `wordCounts`.\n",
    "\n",
    "Utiliza la accion `reduce()` para sumar los conteos en `wordCounts` y entonces divide por el numero de palabras unicas. Para realizar esto primero aplica un `map()` al pair RDD `wordCounts`, que esta formado por tuplas con el formato (key, value), para convertirlo en un RDD de valores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "1.67\n"
     ]
    }
   ],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "from operator import add\n",
    "totalCount = (wordCounts\n",
    "              .map(lambda x: x[1])\n",
    "              .reduce(add))\n",
    "average = totalCount / float(wordCounts.count())\n",
    "print(totalCount)\n",
    "print(round(average, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST Mean using reduce (3b)\n",
    "assert round(average, 2)==1.67, 'incorrect value of average'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 4: Aplicar las funcionalidades desarrolladas a un archivo de texto\n",
    "\n",
    "Para esto hemos de construir una funcion `wordCount`, capaz de trabajar con datos del mundo real que suelen presentan problemas como el uso de mayusculas o minusculas, puntuacion, acentos, etc. Posteriormente, cargar los datos de nuestra fuente de datos y finalmente, calular el conteo de palabras sobre los datos procesados.\n",
    "\n",
    "### (4a) funcion `wordCount`\n",
    "\n",
    "Primero, define una funcion para el conteo de palabras. Deberias reusar las tecnicas que has visto en los apartados anteriores de esta practica. Dicha funcion, ha de tomar un RDD que contenga una lista de palabras, y devolver un pair RDD que contenga todas las palabras con sus correspondientes conteos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('cat', 2), ('elephant', 1), ('rat', 2)]\n"
     ]
    }
   ],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "def wordCount(wordListRDD):\n",
    "    \"\"\"Creates a pair RDD with word counts from an RDD of words.\n",
    "\n",
    "    Args:\n",
    "        wordListRDD (RDD of str): An RDD consisting of words.\n",
    "\n",
    "    Returns:\n",
    "        RDD of (str, int): An RDD consisting of (word, count) tuples.\n",
    "    \"\"\"\n",
    "    return wordListRDD.map(lambda x : (x,1)).reduceByKey(lambda x,y:x+y)\n",
    "\n",
    "print(wordCount(wordsRDD).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST wordCount function (4a)\n",
    "assert sorted(wordCount(wordsRDD).collect())==[('cat', 2), ('elephant', 1), ('rat', 2)],'incorrect definition for wordCount function'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4b) Mayusculas y puntuacion\n",
    "\n",
    "Los ficheros del mundo real son mucho mas complejos que los que hemos estado usando en esta PAC. Algunos de los problemas que son necesarios de solucionar son:\n",
    "  + Las palabras deben de contarse independientemente de si estan en mayuscula o minuscula (por ejemplo, Spark y spark deberian contarse como la misma palabra).\n",
    "  + Todos los signos de puntuacion han de eliminarse.\n",
    "  + Cualquier espacio al principio o al final de la palabra ha de eliminarse.\n",
    "  \n",
    "Define la funcion `removePunctuation` que convierta todo el texto a minusculas, elimine los signos de puntuacion, y elimine los espacios al principio y fin de cada palabra. Usa el modulo de Python [re](https://docs.python.org/2/library/re.html) para eliminar cualquier caracter que no sea una letra, un numero o un espacio.\n",
    "\n",
    "Sino estas familiarizado con las expresiones regulares deberias revisar [este tutorial](https://developers.google.com/edu/python/regular-expressions). Alternativamente, [esta web](https://regex101.com/#python) es de gran ayuda para debugar tus expresiones regulares.\n",
    "\n",
    "**Hints**\n",
    "\n",
    "1. Usa la funcion [re.sub()](https://docs.python.org/2.7/library/re.html#re.sub).\n",
    "2. Para nuestros propositos, \"puntuacion\" significa \"no alphabetico, numerico, o espacio.\" La expresion regular que define estos caracteres es: `[^A-Za-z\\s\\d]`\n",
    "3. No usar `\\W`, ya que retendra los guiones bajos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi you\n",
      "no underscore\n",
      "remove punctuation then spaces\n"
     ]
    }
   ],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "import re\n",
    "def removePunctuation(text):\n",
    "    \"\"\"Removes punctuation, changes to lower case, and strips leading and trailing spaces.\n",
    "\n",
    "    Note:\n",
    "        Only whitespace, letters, and numbers should be retained.  Other characters should should be\n",
    "        eliminated (e.g. it's becomes its).  Leading and trailing spaces should be removed after\n",
    "        punctuation is removed.\n",
    "\n",
    "    Args:\n",
    "        text (str): A string.\n",
    "\n",
    "    Returns:\n",
    "        str: The cleaned up string.\n",
    "    \"\"\"\n",
    "    lowertext = text.lower()\n",
    "    lowertext_rm = re.sub(r'[^a-z0-9\\s]','',lowertext)\n",
    "    lowertext_final = lowertext_rm.strip()\n",
    "    \n",
    "    return lowertext_final\n",
    "    \n",
    "print(removePunctuation('Hi, you!'))\n",
    "print(removePunctuation(' No under_score!'))\n",
    "print(removePunctuation(' *      Remove punctuation then spaces  * '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST Capitalization and punctuation (4b)\n",
    "assert removePunctuation(\" The Elephant's 4 cats. \") == 'the elephants 4 cats', 'incorrect definition for removePunctuation function'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4c) Cargar un fichero de texto\n",
    "\n",
    "Para la siguiente parte, usaremos el texto ya mencionado Lorem Ipsum generado para la pràctica.Para convertir un fichero de texto en un RDD, usaremos el metodo `SparkContext.textFile()`. Tambien usaremos la funcion que acabamos de crear `removePunctuation()` dentro de una transformacion `map()` para eliminar todos los caracteres no alphabeticos, numericos or espacios. Dado que el fichero es bastante grandre, usaremos `take(15)`, de forma que tan solo imprimiremos por pantalla las 15 primeras lineas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%fs` not found.\n"
     ]
    }
   ],
   "source": [
    "%fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aut minima deleniti et autem minus illo esse dolores eligendi corrupti dolore minima nostrum eos nobis nam nihil aspernatur nam ut quae sint laborum ut dolores error possimus aperiam consequatur',\n",
       " 'pariatur sed quo non itaque qui pariatur saepe ad quis consequatur nihil iste molestias et eos ut expedita vel reiciendis dolorem enim doloribus quam architecto aperiam',\n",
       " 'sed repudiandae pariatur similique est aut sequi animi in aperiam enim ipsa enim dolorem inventore aut quo odio in consequatur et',\n",
       " 'aspernatur ad esse et aliquid itaque dolores rerum quia commodi explicabo non magnam nostrum consectetur non sint eum nulla et aut quis doloribus itaque nulla molestiae quis est est quo facilis incidunt a ipsa in itaque sed aut nobis facere dignissimos atque unde cum ea vero',\n",
       " 'tenetur vel quod voluptatum laudantium dolores neque aut est modi qui aperiam itaque aperiam quae ratione doloremque aut delectus quas qui',\n",
       " 'qui placeat vel ipsam praesentium sint recusandae dicta minus praesentium omnis sequi a sed veritatis porro ab et officia esse commodi pariatur sequi cumque',\n",
       " 'mollitia facilis amet deleniti quia laborum commodi et molestias maxime quia dignissimos inventore neque libero deleniti ad quo corrupti numquam quis accusantium',\n",
       " 'architecto harum sunt et enim nisi commodi et id reprehenderit illum molestias illo facilis fuga eum illum quasi fugit qui',\n",
       " 'modi voluptatem quia et saepe inventore sed quo ea debitis explicabo vel perferendis commodi exercitationem sequi eum dolor cupiditate ab molestiae nemo ullam neque hic ipsa cupiditate dolor molestiae neque nam nobis nihil mollitia unde',\n",
       " 'voluptates quod in ipsum dicta fuga voluptatibus sint consequatur quod optio molestias nostrum repellendus consequatur aliquam fugiat provident omnis minus est quisquam exercitationem eum voluptas fugit quae eveniet perspiciatis assumenda maxime']"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tan solo ejecuta este codigo\n",
    "import os.path\n",
    "\n",
    "fileName = os.path.join('/aula_B2.585/data/', 'LoremIpsum.txt')\n",
    "\n",
    "loremRDD = sc.textFile(fileName, 8).map(removePunctuation).filter(lambda x: len(x)>0)\n",
    "loremRDD.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4d) Extraer las palabras de las lineas\n",
    "\n",
    "Antes de poder usar la funcion `wordcount()`, hemos de solucionar dos problemas con el formato del RDD:\n",
    "  + El primer problema es que necesitamos dividir cada linea por sus espacios. ** Esto lo solucionaremos en el apartado (4d). **\n",
    "  + El segundo problema es que necesitamos filtar las lineas completamente vacias. ** Esto lo solucionaremos en el apartado (4e). **\n",
    "\n",
    "Para aplicar una transformacion que divida cada elemento del RDD por sus espacios, hemos de aplicar la funcion incorporada en los strings de Python [split()](https://docs.python.org/2/library/string.html#string.split). Cuidado que a primera vista puede parecer que la funcion necesaria es una transformacion `map()`, pero si piensas un poco mas sobre el resultado de la funcion `split()` te daras cuenta que esta no es la opcion correcta.\n",
    "\n",
    "> Nota:\n",
    "> * No uses la implementacion estandar del `split()`, pasale un valor de separacion. Por ejemplo, para dividir `line` por comas, usa `line.split(',')`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['voluptatum', 'voluptatum', 'voluptatum', 'voluptatum', 'voluptatum']\n",
      "29249699\n"
     ]
    }
   ],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "loremWordsRDD = loremRDD.flatMap(lambda x: x.split(\" \"))\n",
    "loremWordsCount = loremWordsRDD.count()\n",
    "print(loremWordsRDD.top(5))\n",
    "print(loremWordsCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST Words from lines (4d)\n",
    "# This test allows for leading spaces to be removed either before or after\n",
    "# punctuation is removed.\n",
    "assert loremWordsCount == 29249699, 'incorrect value for loremWordsCount'\n",
    "assert loremWordsRDD.top(5)==[u'voluptatum', u'voluptatum', u'voluptatum', u'voluptatum', u'voluptatum'], 'incorrect value for loremWordsRDD'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4e) Calcula palabras distintas\n",
    "\n",
    "El siguiente paso es contar cuantas palabras distintas contiene nuestro texto. Puedes usar las transformaciones map() y reduceByKey() ya utilizadas anteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tempora', 'sapiente', 'vitae', 'nisi', 'quidem', 'consectetur', 'perferendis', 'debitis']\n",
      "182\n"
     ]
    }
   ],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "\n",
    "distintWordsMapRDD = loremWordsRDD.map(lambda x:(x,1)).reduceByKey(lambda x,y:x+y)\n",
    "\n",
    "distintWordsRDD=distintWordsMapRDD.keys().distinct()\n",
    "\n",
    "print(distintWordsRDD.take(8))   \n",
    "print(distintWordsRDD.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST Remove empty elements (4e)\n",
    "assert distintWordsRDD.count()== 182, 'incorrect value for shakeWordCount'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tempora', 'sapiente', 'vitae', 'nisi', 'quidem', 'consectetur', 'perferendis', 'debitis', 'sunt', 'porro', 'ipsum', 'fugit', 'eligendi', 'aspernatur', 'eius', 'fuga', 'placeat', 'consequatur', 'illo', 'earum', 'impedit', 'illum', 'officiis', 'nulla', 'accusantium', 'qui', 'harum', 'pariatur', 'libero', 'reiciendis', 'totam', 'autem', 'occaecati', 'inventore', 'consequuntur', 'ipsam', 'ut', 'repellendus', 'velit', 'id', 'voluptatem', 'corrupti', 'a', 'voluptas', 'itaque', 'nihil', 'animi', 'iure', 'repellat', 'adipisci', 'assumenda', 'temporibus', 'at', 'repudiandae', 'nostrum', 'tenetur', 'atque', 'ratione', 'voluptate', 'cumque', 'reprehenderit', 'sint', 'provident', 'amet', 'iusto', 'vero', 'minus', 'iste', 'quia', 'possimus', 'est', 'quae', 'odio', 'neque', 'accusamus', 'sequi', 'quo', 'mollitia', 'aut', 'excepturi', 'necessitatibus', 'praesentium', 'laboriosam', 'natus', 'minima', 'doloremque', 'delectus', 'aperiam', 'voluptatum', 'eaque', 'doloribus', 'nemo', 'veritatis', 'saepe', 'laborum', 'quaerat', 'officia', 'in', 'distinctio', 'eveniet', 'aliquid', 'ex', 'beatae', 'soluta', 'ipsa', 'rerum', 'fugiat', 'suscipit', 'modi', 'omnis', 'magnam', 'dicta', 'dolores', 'quam', 'rem', 'sit', 'quod', 'culpa', 'et', 'ullam', 'dolor', 'corporis', 'voluptates', 'non', 'expedita', 'quasi', 'incidunt', 'numquam', 'error', 'quisquam', 'cum', 'laudantium', 'odit', 'enim', 'alias', 'nesciunt', 'dignissimos', 'molestias', 'eos', 'quos', 'eum', 'ab', 'quis', 'labore', 'molestiae', 'maiores', 'voluptatibus', 'commodi', 'exercitationem', 'deserunt', 'similique', 'sed', 'quas', 'optio', 'unde', 'explicabo', 'architecto', 'ducimus', 'cupiditate', 'veniam', 'deleniti', 'asperiores', 'dolorum', 'tempore', 'ad', 'nobis', 'esse', 'facere', 'perspiciatis', 'nam', 'aliquam', 'facilis', 'quibusdam', 'vel', 'blanditiis', 'dolore', 'dolorem', 'magni', 'recusandae', 'ea', 'maxime', 'hic']\n"
     ]
    }
   ],
   "source": [
    "print(distintWordsRDD.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4f) Cuenta las palabras\n",
    "\n",
    "Ahora que tenemos un RDD que contiene solo palabras. El siguiente paso es aplicar la funcion `wordCount()` para producir una lista con los conteos de palabras. Podemos ver las 15 mas comunes usando la accion `takeOrdered()`; sin embargo, como los elementos del RRD son pares, necesitamos una funcion especial que ordene los pares de la forma correcta.\n",
    "\n",
    "Usa las funciones  `wordCount()` y `takeOrdered()` para obtener las 15 palabras mas comunes junto con sus conteos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('et', 1291567), ('aut', 705322), ('ut', 704966), ('qui', 704703), ('est', 587782), ('voluptatem', 469634), ('quia', 469401), ('rerum', 353054), ('sed', 352873), ('omnis', 352653), ('consequatur', 352436), ('sit', 352396), ('non', 351959), ('voluptas', 351127), ('dolorem', 235822)]\n"
     ]
    }
   ],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "top15WordsAndCounts = wordCount(loremWordsRDD).takeOrdered(15, key = lambda x: -x[1])\n",
    "print(top15WordsAndCounts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST Count the words (4f)\n",
    "assert top15WordsAndCounts== [('et', 1291567), ('aut', 705322), \n",
    "                              ('ut', 704966), ('qui', 704703), \n",
    "                              ('est', 587782), ('voluptatem', 469634), \n",
    "                              ('quia', 469401), ('rerum', 353054), ('sed', 352873),\n",
    "                              ('omnis', 352653), ('consequatur', 352436), \n",
    "                              ('sit', 352396), ('non', 351959), ('voluptas', 351127), \n",
    "                              ('dolorem', 235822)],'incorrect value for top15WordsAndCounts'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 5: Calcular algunos estadisticos\n",
    "\n",
    "Usando las mismas tecnicas que has aplicado en los ejercicios anteriores responde a las siguientes preguntas:\n",
    "\n",
    "### (5a) ¿Cual es la longitud media de todas las palabras (no repetidas) que aparecen en el texto?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La longitud media de las palabras es 6.42\n"
     ]
    }
   ],
   "source": [
    "#Calculamos la longitud total de todas las palabras (no repetidas)\n",
    "\n",
    "len_allwords = distintWordsRDD.map(lambda x : len(x)).reduce(add)\n",
    "\n",
    "#Teniendo la longitud total lo dividimos para el numero de palabras y obtendremos la longitud media\n",
    "\n",
    "average_allwords = len_allwords/float(distintWordsRDD.count())\n",
    "\n",
    "print(\"La longitud media de las palabras es \" + str(round(average_allwords,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5b) ¿Cuantas palabras distintas hay con al menos una 'a' y al menos una 'u' ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "En el texto seleccionado nos encontramos con 44 palabras diferentes que al menos contienen una a y u.\n"
     ]
    }
   ],
   "source": [
    "Words_a_u = distintWordsRDD.filter(lambda x: re.match(r'(\\b\\w*[a]\\w*[u]\\w*\\b)|(\\b\\w*[u]\\w*[a]\\w*\\b)',x))\n",
    "\n",
    "print(\"En el texto seleccionado nos encontramos con {} palabras diferentes que al menos contienen una a y u.\".format(Words_a_u.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5c) ¿Cuál es la palabra que más se repite y la que menos?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "MapReduceWords = loremWordsRDD.map(lambda x: (x,1)).reduceByKey(lambda x,y:x+y)\n",
    "WordsAndCounts=MapReduceWords.takeOrdered(MapReduceWords.count(),key = lambda x: -x[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La palabra mas repetida sería 'et' con 1291567 veces\n",
      "La palabra menos repetida sería 'esse' con 116586 veces\n"
     ]
    }
   ],
   "source": [
    "print(\"La palabra mas repetida sería '{}' con {} veces\".format(WordsAndCounts[0][0],WordsAndCounts[0][1]))\n",
    "print(\"La palabra menos repetida sería '{}' con {} veces\".format(WordsAndCounts[-1][0],WordsAndCounts[-1][1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "name": "Lab_1_WordCount",
  "notebookId": 1529163670453288
 },
 "nbformat": 4,
 "nbformat_minor": 4
}